## 気づき

### ベクトル検索

- ベクトルの次元数は、埋め込みに使用するモデルに依存する。
  - 例えば, google の `models/embedding-001` は 768次元のベクトルを出力する。
- クエリのベクトルと、埋め込みされた文章のベクトルを比較するためには、ベクトルの次元数を揃える必要がある。
  - ゆえに、クエリを処理するモデルと、埋め込みされた文章を処理するモデルは、同じモデルを使用するのが無難。


### Qdrant

- Qdrant  はベクトルデータと、そのメタデータを合わせて保存することができる。
  - クエリはベクトルデータを使うこともできるが、メタデータを使うこともできて便利。

### pydantic

- llm の回答を pydantic というライブラリを使って構造化データに変換することができる。
  - pydantic は、データのバリデーションやシリアライズを行うライブラリ。
  - このようなライブラリを使うことで、API のレスポンスを構造化することができた。


### llamaindex

- RAGを実装するのに必要な概念を抽象化して提供してくれる高度なフレームワークという印象。


## その他

### OpneAI ではなく Google のモデルを使用した理由

- 無料で利用できる
- llamaindex で openai 以外のモデルを利用する手順を確認したかった

## 今後やってみたいこと

- データストアにGraphDBを導入し、GraphRAGをためす
  - Neo4j など
- llm のモデルをローカルLLMに変更する
  - ローカルLLMの構築から、llamaindex での利用までの手順を確認する
